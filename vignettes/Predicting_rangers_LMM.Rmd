---
title: "Predicting the number of rangers using LMM"
output:
  rmarkdown::html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    code_folding: hide
vignette: >
  %\VignetteIndexEntry{Number_rangers_LMM}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---

<style type="text/css">
  .main-container {
    max-width: 1200px;
    margin-left: auto;
    margin-right: auto;
  }
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  error = TRUE,
  comment = "#>",
  cache = FALSE, # don't put TRUE as inline chunk make R CMD check to crash
  cache.path = "cache/cache_vignette_predicting_rangers_LMM/",
  fig.path = "figs/fig_vignette_predicting_rangers_LMM/",
  fig.align = 'center'
)
```

```{r setup, message=FALSE}
library(rangeRinPA)
library(dplyr)
library(tidyr)
library(ggplot2)
library(broom)
library(gt)
library(spaMM)
```

## Using only the surveyed area of terrestrial PAs in the country as predictor

At the level of countries, the *surveyed area of terrestrial PAs in the country* (**PA_area_surveyed**) is a very good predictor for the number of rangers (**staff_rangers** for short).
I will first characterise the relationship between PA_area_surveyed and staff_rangers before using it to predict the number of rangers in countries for which we don't have it.

### Data preparation

We will use a version of our dataset without Greenland in same cases since this country is a clear outlier.

```{r data_no_greenland, warning=FALSE}
data_rangers %>%
  filter(countryname_eng != "Greenland") -> data_rangers_noG

rownames(data_rangers_noG) <- data_rangers_noG$countryname_iso
```

### Relationship between PA_area_surveyed and staff_rangers

On the original scale, the relationship is not very obvious:

```{r PA_area_surveyed plot1, fig.height=6, fig.width=6, cache=TRUE}
ggplot(data_rangers) +
  aes(y = staff_rangers, x = PA_area_surveyed, label = countryname_iso, colour = country_UN_continent) +
  geom_text() +
  scale_colour_discrete(guide = "none") +
  theme_minimal()
```

but when plotting the variable on a log scale, the relation appears more clearly:

```{r PA_area_surveyed plot2, fig.height=6, fig.width=6, cache=TRUE}
ggplot(data_rangers) +
  aes(y = staff_rangers, x = PA_area_surveyed, label = countryname_iso, colour = country_UN_continent) +
  geom_text() +
  coord_trans(x = "log", y = "log1p") +
  scale_x_continuous(breaks = 10^(0:7), minor_breaks = NULL, limits = c(1, 1e7)) +
  scale_y_continuous(breaks = 10^(0:7), minor_breaks = NULL) +
  scale_colour_discrete(guide = "none") +
  theme_minimal()
```

**That the relationship is (roughly) linear on a log-log plot, suggests an allometric relationship and not a proportional relationship**.

Let's start by showing what is wrong about assuming that the relationship is proportional:

```{r PA_area_surveyed plot3, fig.height=6, fig.width=6, cache=TRUE}
ggplot(data_rangers_noG) +
  aes(y = PA_area_surveyed / staff_rangers, x = PA_area_surveyed, label = countryname_iso, colour = country_UN_continent) +
  geom_text() +
  coord_trans(x = "log", y = "log1p") +
  scale_x_continuous(breaks = 10^(0:7), minor_breaks = NULL, limits = c(1, 1e7)) +
  scale_y_continuous(breaks = 10^(0:7), minor_breaks = NULL) +
  scale_colour_discrete(guide = "none") +
  theme_minimal()
```

As you can see in the previous plot, the area of PA per ranger increases sharply with PA_area_surveyed. So we cannot say that one ranger covers a fixed given area. For the same reason, we cannot say that there is x rangers per square-km of PAs.

In contrast, owing to the particular nature of the allometric relationship we have (more on that later), what is fixed is the ratio of the log quantities:

```{r PA_area_surveyed plot4, fig.height=6, fig.width=6, cache=TRUE}
ggplot(data_rangers_noG) +
  aes(y = log(staff_rangers + 1) / log(PA_area_surveyed), x = PA_area_surveyed, label = countryname_iso, colour = country_UN_continent) +
  geom_text() +
  coord_trans(x = "log") +
  scale_x_continuous(breaks = 10^(0:7), minor_breaks = NULL, limits = c(1, 1e7)) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), minor_breaks = NULL) +
  scale_colour_discrete(guide = "none") +
  theme_minimal()
```

As you can see, the ratio of the log no longer depend on PA_area.
(In case you wonder, the $+1$ is there so that to avoid log(0) which are not defined).


Let's now fit the allometric relationship using a linear model:

$\mathrm{log}(\mathrm{staff\_rangers} + 1) = a \times \mathrm{log}(\mathrm{PA\_area\_surveyed}) + b$ (model 1).

Here are the estimates we obtain:

```{r lm}
fit_allometry <- lm(log(staff_rangers + 1) ~  log(PA_area_surveyed), data = data_rangers_noG)
tidy(summary(fit_allometry)) %>%
  mutate(across(where(is.numeric), .fns = round, digits = 3)) %>%
  gt()
```

as you can see, the intercept ($b$) is no different from 0, so we can simplify the model to:

$\mathrm{log}(\mathrm{staff\_rangers} + 1) = a \times \mathrm{log}(\mathrm{PA\_area\_surveyed})$ (model 2).

This leads to the following estimates:

```{r lm2}
fit_allometry2 <- lm(log(staff_rangers + 1) ~  log(PA_area_surveyed) - 1, data = data_rangers_noG)
tidy(summary(fit_allometry2)) %>%
  mutate(across(where(is.numeric), .fns = round, digits = 3)) %>%
  gt()
```

Dropping the intercept is more rigorously justified by the fact that doing so improves the predictive ability of the model (AIC model 1 = `r round(AIC(fit_allometry), digits = 1)`; AIC model 2 = `r round(AIC(fit_allometry2), digits = 1)`).

Fitting simple LM thus confirms that we do not need to consider an intercept and it leads to the following relationship:

$\large{\mathrm{staff\_rangers} + 1 = \mathrm{PA\_area\_surveyed}^{`r round(coef(fit_allometry2)[[1]], digits = 3)`}}$.

This simple linear models meets well the assumptions of linear regressions (homoscedasticity, independence and normality):

```{r plot lm fit, fig.height=10, fig.width=10, cache=TRUE}
par(mfrow = c(2, 2))
plot(fit_allometry2)
```

Another way of fitting an linear model is to directly fit the ratio: 

$\dfrac{\mathrm{log}(\mathrm{staff\_rangers} + 1)}{\mathrm{log}(\mathrm{PA\_area\_surveyed})} = a$ (model 3).

```{r lm3}
fit_allometry3 <- lm(log(staff_rangers + 1)/log(PA_area_surveyed) ~ 1, data = data_rangers_noG)
tidy(summary(fit_allometry3)) %>%
  mutate(across(where(is.numeric), .fns = round, digits = 3)) %>%
  gt()
```

The model 2 & 3 are not strictly equivalent because they impose different structure in terms of the residual variation.

This is because if

$Y/X = \beta_0 + \mathrm{homoscedastic\ residuals}$

then

$Y = \beta_0 \times X + X \times \mathrm{homoscedastic\ residuals}$

or

$Y = \beta_0 \times X + \mathrm{heteroscedastic\ residuals}$

We can use this result to rewrite the model 3 under a form that is comparable to model 2, and compare the 2 models

```{r comparing models}
fit_model2 <- fitme(I(log(staff_rangers + 1)/log(PA_area_surveyed)) ~ 1, data = data_rangers_noG)
fit_model3 <- fitme(I(log(staff_rangers + 1)/log(PA_area_surveyed)) ~ 1, prior.weights = sqrt(1/log(PA_area_surveyed)), data = data_rangers_noG)
```

The AIC of the model 2 expressed in such a way is `r round(AIC(fit_model2))` and that of the model 3 is `r round(AIC(fit_model3))`, hence the model 2 is much better and we will use it as a basis for our analyses.

Let's now visualise the fit of model 2 on the raw data:

```{r area_PA_total with PI plot2, fig.height=6, fig.width=6, cache=TRUE}
x <- 10^(seq(0, 7, length = 1000))
pred_for_plot <- as.data.frame(predict(fit_allometry2, newdata = data.frame(PA_area_surveyed = x), interval = "prediction"))
pred_for_plot$PA_area_surveyed <- x

CI_for_plot <- as.data.frame(predict(fit_allometry2, newdata = data.frame(PA_area_surveyed = x), interval = "confidence"))
CI_for_plot$PA_area_surveyed <- x

ggplot(data_rangers) +
  aes(y = staff_rangers, x = PA_area_surveyed, label = countryname_iso, colour = country_UN_continent) +
  geom_text() +
  geom_line(aes(y = exp(fit) - 1, x = PA_area_surveyed), data = pred_for_plot, inherit.aes = FALSE) +
  geom_line(aes(y = exp(upr) - 1, x = PA_area_surveyed), data = pred_for_plot, inherit.aes = FALSE, linetype = "dashed") +
  geom_line(aes(y = exp(lwr) - 1, x = PA_area_surveyed), data = pred_for_plot, inherit.aes = FALSE, linetype = "dashed") +
  geom_line(aes(y = exp(upr) - 1, x = PA_area_surveyed), data = CI_for_plot, inherit.aes = FALSE, linetype = 6) +
  geom_line(aes(y = exp(lwr) - 1, x = PA_area_surveyed), data = CI_for_plot, inherit.aes = FALSE, linetype = 4) +
  coord_trans(x = "log1p", y = "log1p") +
  scale_x_continuous(breaks = 10^(0:7), minor_breaks = NULL, limits = c(0, 1e7)) +
  scale_y_continuous(breaks = 10^(0:7), minor_breaks = NULL, limits = c(0, 1e5)) +
  scale_colour_discrete(guide = "none") +
  theme_minimal()
```

The larger interval is the 95% prediction interval (i.e. where new data may be) and the small one the 95% confidence interval (i.e. where true relationship may be).


### Predictive power

We now turn to the predictive power (on the fitted data). So far, so good:

```{r plot lm pred, fig.height=6, fig.width=6, cache=TRUE}
data_rangers_noG %>%
  mutate(predicted_rangers = data_rangers_noG$PA_area_surveyed^coef(fit_allometry2)[[1]] - 1,
         observed_rangers = data_rangers_noG$staff_rangers) -> data_rangers_noG2

data_rangers_noG2 %>%
  filter(!is.na(observed_rangers)) -> data_rangers_noG2_obs

data_rangers_noG2 %>%
  filter(is.na(observed_rangers)) -> data_rangers_noG2_notobs


ggplot(data_rangers_noG2_obs) +
  aes(y = observed_rangers, x = predicted_rangers, label = countryname_iso, colour = country_UN_continent) +
  geom_text() +
  geom_abline(slope = 1) +
  coord_trans(x = "log1p", y = "log1p") +
  scale_x_continuous(breaks = 10^(0:7), minor_breaks = NULL, limits = c(1, 1e5)) +
  scale_y_continuous(breaks = 10^(0:7), minor_breaks = NULL, limits = c(1, 1e5)) +
  labs(y = "Observed staff_rangers", x = "Predicted staff_rangers") +
  scale_colour_discrete(guide = "none") +
  theme_minimal()
```

The Pearson correlation between the predicted and the observed `log(staff_rangers + 1)` is actually pretty high: r = `r round(cor(log(data_rangers_noG2_obs$predicted_rangers + 1), log(data_rangers_noG2_obs$observed_rangers + 1)), digits = 2)`.

This is good, very good, but also a little misleading since being a little off on a log scale can mean a lot.

The Pearson correlation between the predicted and the observed number of rangers (not logged) is lower due to the non-linearity: r = `r round(cor(data_rangers_noG2_obs$predicted_rangers, data_rangers_noG2_obs$observed_rangers), digits = 2)`.
A non-parametric correlation retrieves a high value: rho (Spearman) = `r round(cor(data_rangers_noG2_obs$predicted_rangers, data_rangers_noG2_obs$observed_rangers, method = "spearman"), digits = 2)`.

Another way to look at the predictive power on the original scale is to compute the sum of rangers for the country for which we know it and compare that to the observations (nb: those numbers do not include Greenland):

```{r predictive sum}
gt(tibble(total_predicted = sum(data_rangers_noG2_obs$predicted_rangers),
          total_observed = sum(data_rangers_noG2_obs$observed_rangers)))
```

That means that we predict `r round(100* sum(data_rangers_noG2_obs$predicted_rangers) /  sum(data_rangers_noG2_obs$observed_rangers), digits = 1)` % of the observed sum.
The reason why it is a little lower is that the prediction underestimate the countries with the largest number of rangers. For example, in India alone, the model predicts `r round(data_rangers_noG2_obs[data_rangers_noG2_obs$countryname_iso == "IND", "predicted_rangers", drop = TRUE])` instead of `r format(data_rangers_noG2_obs[data_rangers_noG2_obs$countryname_iso == "IND", "staff_rangers", drop = TRUE], scientific = FALSE)`.

So, is this model good or not? The result on the sum is quite scary, but this is because I did not yet consider the uncertainty on the predictions. In particular, I used the same value for the coefficient of allometry for predicting all countries, but I did show you above that there is quite a lot of uncertainty around this coefficient. For example, on the real data, India does not have a ratio of `r round(coef(fit_allometry3)[[1]], digits = 3)` but of `r round(data_rangers %>% filter(countryname_iso == "IND") %>% mutate(ratio = log(staff_rangers + 1)/log(PA_area_surveyed)) %>% pull(ratio), digits = 3)`, which is quite different!

### Predictions

I am currently revising the approach so everything else has been moved for now in the vignette called Predicting_rangers_LMM_BROKEN which is NOT compilable.
